---
title: "Tarea 8"
output: html_document
---

# Tarea final

# Estudiante
* Merian Herrera Fuentes
* meryann68@gmail.com
* 207180364

```{r warning=FALSE, echo=FALSE}
library(caret)
library(caTools)
library(class)
library(corrplot)
library(dplyr)
library(e1071)
library(ggplot2)
library(GGally)
library(lattice)
library(lessR)
library(Metrics)
library(randomForest)
library(readr)
library(ROCR)
library(rpart)
library(rpart.plot)
library(tidyr)
library(glue)
```

## Desarrolla una pregunta

 A continuacion se va  a trabajar con los datos abiertos del ministerio de salud, especificicamente informacion de tumores malignos. La información de incidencia por tumores malignos es infomacion que el SIRNAT pone a disposición, la cual ha sido elaborada por el Registro Nacional de Tumores, a partir de las notificaciones que envían los establecimientos de salud públicos y privados. La notificación obligatoria del cáncer en Costa Rica fue establecida por decreto ejecutivo desde 1976 y reafirmada por el Reglamento de Vigilancia de la Salud vigente actualmente.

Basados en los datos recopilados por el SIRNAT, se desea **predecir el tipo de tumor** que un paciente puede presentar basadas en sus caracteristicas, asi mismo se desea identificar los tipos de tumores mas comunes en las diferentes provicias de Costa Rica.

Fuente de los datos:

http://datosabiertos.saluddigital.cr/#

Nota: En la pagina del ministerio de salud buscar el apartado tumores, y generar la tabla con todos los annos y todas las regiones.

## Prepara los datos

```{r}
tumores_headers = c("numero_caso", "codigo_subcapitulo", "subcapitulo", "codigo_dignostico", "diagnostico", "resultado_diagnostico", "sexo", "edad_quinquenal", "edad", "fecha_diagnostico", "provincia", "canton", "distrito", "region_ministerio_salud", "area_ministerio_salud", "region_ccss", "area_ccss", "codigo_establecimiento", "establecimiento", "poblacion_provincia", "poblacion_canton", "poblacion_distrito", "fecha_diagnostico_2", "anno")

variables_tumores <- c("diagnostico", "resultado_diagnostico", "sexo", "edad_quinquenal", "provincia", "canton", "distrito", "establecimiento", "fecha_diagnostico_2", "anno")


tumores <- read.csv("tumores.csv", 
                  encoding = "UTF-8",
                  fileEncoding = "UTF-8",
                  header = TRUE,
                  sep = ";")

tumores <- tumores[, (names(tumores) %in% variables_tumores)]

head(tumores, 800)
```

- **Diagnostico**

```{r}
tumores %>% 
  group_by(diagnostico) %>% 
  summarise(count = n()) 
```

- **Resultado Diagnostico**

```{r}
tumores %>% 
  group_by(resultado_diagnostico) %>% 
  summarise(count = n()) 
```

- **Sexo**

```{r}
tumores %>% 
  group_by(sexo) %>% 
  summarise(count = n()) 
```

- **Edad Quinquenal**

```{r}
tumores %>% 
  group_by(edad_quinquenal) %>% 
  summarise(count = n()) 
```

- **Provincia**

```{r}
tumores %>% 
  group_by(provincia) %>% 
  summarise(count = n()) 
```

- **Establecimiento**

```{r}
tumores %>% 
  group_by(establecimiento) %>% 
  summarise(count = n()) 
```

- **Anno**

```{r}
tumores %>% 
  group_by(anno) %>% 
  summarise(count = n()) 
```

- **Canton**

```{r}
tumores %>% 
  group_by(canton) %>% 
  summarise(count = n()) 
```

- **Distrito**

```{r}
tumores %>% 
  group_by(distrito) %>% 
  summarise(count = n()) 
```

**DATOS NUlOS**

```{r}
sapply(tumores, function(x) {sum(is.na(x))})
```

> No se encuentran datos nulos en las variabled del dataset.

Nota: Debido a que el set de datos original se encontraba codificado por ejemplo en la variable provincia se encontraba *1 - SAN JOSE* y tambien *^1* para referirse a la misma provincia, se han procesado esos datos usando Python, se adjunta el notebook de jupyter en el .zip, asimismo, se adjunta el csv de datos original.

Como se puede observar en la exploracion de datos anterior, el dataset obtenido de los datos recopilados por el SIRNAT, no contiene la informacion esperada, no se encuentra una variable para el tipo de tumor maligno con el que fue diagnosticado el paciente, si no mas bien solo podemos encontrar una columna diagnostico donde encontramos diagnosticos de 547 enfermedades diferentes, de las cuales muchas de ellas no se relacionan con tumores si no mas bien son enfermedades como diabetes, tuberculosis, dolor ocular, hipertension...

Por lo tanto, se necesita cambiar la pregunta planteada al inicio de este documento, ya que los datos obtenidos son insuficientes. Ahora, se va a predecir el resultado de los diagnosticos, ya que los mismos pueden ser:
    - Confirmado
    - Descartado
    - Probable
    - Sospechoso

Se han seleccionado los siguientes modelos de clasificacion para trabajar con los datos:
    - Arbol de decision
    - SVN
    

### Preparacion de los datos

```{r}
tumores <- tumores %>% separate(resultado_diagnostico, 
                c("resultado"))

tumores$resultado <-  as.factor(with(tumores, ifelse(tumores$resultado %in% c("DESCARTADO", "PROBABLE", "SOSPECHOSO"), "NO CONFIRMADO", "CONFIRMADO")))
```


```{r}
tumores %>% 
  group_by(resultado) %>% 
  summarise(count = n()) 
```

De antemano se observa que los datos estan desproporcionados ya que la mayoria de los resultados se encuentran clasificados como *confirmado*, lo cual va a introducir el fenomeno de sesgo en nuestros modelos. Sin embargo, como los datos encontrados en la plataforma de datos abiertos son insuficientes para trabajar un modelo en las condiciones optimas, se van a crear los modelos planteados anteriormente a modo de ejemplo, para efectos de la tarea.


```{r}
glimpse(tumores)
```

#### Graficos de barras para mostrar relaciones entre algunas de las variables con el resultado.

```{r}
provincia_resultado  <- tumores %>% 
  group_by(provincia, resultado) %>% 
  summarise(count = n()) 

ggplot(data = provincia_resultado, aes(x = provincia, y = count, fill = resultado)) +
  geom_bar(stat = "identity")
```

```{r}
sexo_resultado  <- tumores %>% 
  group_by(sexo, resultado) %>% 
  summarise(count = n()) 

ggplot(data = sexo_resultado, aes(x = sexo, y = count, fill = resultado)) +
  geom_bar(stat = "identity")
```

```{r}
edad_quinquenal_resultado  <- tumores %>% 
  group_by(edad_quinquenal, resultado) %>% 
  summarise(count = n()) 

ggplot(data = edad_quinquenal_resultado, aes(x = edad_quinquenal, y = count, fill = resultado)) +
  geom_bar(stat = "identity")
```

## Desarrollo modelos

#### Division de los datos

```{r}
set.seed(12)
dividir_datos <- sample.split(tumores$resultado, SplitRatio = 0.7)

tumores_entrenamiento <- tumores[dividir_datos,]
tumores_prueba <- tumores[!dividir_datos,]

dividir_entrenamiento <- sample.split(tumores_entrenamiento$resultado, SplitRatio = 0.1)

tumores_entrenamiento_10 <- tumores_entrenamiento[dividir_entrenamiento,]

```

```{r}
barplot(table(tumores_entrenamiento$resultado), main = 'Distribución de las clases en los Datos de Entrenamiento', ylab = 'Observaciones', xlab = 'Clase')

barplot(table(tumores_prueba$resultado), main = 'Distribución de las clases en los Datos de Prueba', ylab = 'Observaciones', xlab = 'Clase')
```

#### Creacion de los modelos

##### **Arbol de Decision**

```{r}

dataset_modelos = tumores_entrenamiento

modelo_arbol <- rpart(resultado ~ sexo + provincia + edad_quinquenal, 
                      data = dataset_modelos, 
                      method =  'class', 
                      control = rpart.control(cp = 0))

rpart.plot(modelo_arbol,
           box.palette = "blue",
           main = "Clasificacion de los resultados de enfermedades")
```

##### SVM

```{r}
modelo_svn <- svm(resultado ~ sexo + provincia + edad_quinquenal, 
                  data = dataset_modelos, 
                  kernel = 'linear',
                  cross = 2, 
                  scale = FALSE)
summary(modelo_svn)

```

#### Evaluacion de los modelos

```{r}
calcular_metricas_modelo <- function(verdaderos_positivos, verdaderos_negativos, falsos_positivos, falsos_negativos, matriz_confusion) {
   # VP+VN / Total
  exactitud <- round((verdaderos_positivos + verdaderos_negativos) / sum(matriz_confusion) * 100, digits = 2)
  
  # VP / total positivos
  sensibilidad <- round(verdaderos_positivos / (verdaderos_positivos + falsos_negativos) * 100, digits = 2)
  
  # VP / Total clasificados positivos
  precision <- round(verdaderos_positivos / (verdaderos_positivos + falsos_positivos ) * 100, digits = 2)
  
  # VN/ Total Negativos
  especificidad <- round(verdaderos_negativos / (verdaderos_negativos + falsos_positivos) * 100, digits = 2)
  
  print(glue("* Exactitud Total del modelo: {exactitud}% "))
  print(glue("* Sensibilidad del modelo: {sensibilidad}% "))
  print(glue("* Precision del modelo: {precision}% "))
  print(glue("* Especificidad del modelo: {especificidad}% "))
}

```


##### SVM

```{r}
dataset_evaluacion = tumores_prueba

predicciones_svn <- predict(modelo_svn, 
                                   newdata = dataset_evaluacion)

matriz_svn <- table(predicciones_svn, dataset_evaluacion$resultado)

verdaderos_positivos_svn <- matriz_svn[2,2]
verdaderos_negativos_svn <- matriz_svn[1,1]
falsos_positivos_svn <- matriz_svn[1,2]
falsos_negativos_svn <- matriz_svn[2,1]

calcular_metricas_modelo(verdaderos_positivos_svn, verdaderos_negativos_svn, falsos_positivos_svn, falsos_negativos_svn, matriz_svn)

```

##### Arbol de decision

```{r}
predicciones_arbol <- predict(modelo_arbol, 
                              newdata = dataset_evaluacion, 
                              type = 'class')

# Comparar la etiqueta verdadera contra la etiqueta predicha.
matriz_arbol <- table(predicciones_arbol, dataset_evaluacion$resultado)

verdaderos_positivos_arbol <- matriz_arbol[2,2]
verdaderos_negativos_arbol <- matriz_arbol[1,1]
falsos_positivos_arbol <- matriz_arbol[1,2]
falsos_negativos_arbol <- matriz_arbol[2,1]

calcular_metricas_modelo(verdaderos_positivos_arbol, verdaderos_negativos_arbol, falsos_positivos_arbol, falsos_negativos_arbol, matriz_arbol)

```

#### Conclusiones

- **Para el modelo SVN**, se calcula una exactitud total del 94.46%, lo cual significa que el 94.46% de los valores fueron clasificados correctamente. Sin embargo, no se considera un modelo confiable debido al sesgo que presentan los datos.

Con respecto al tipo de decisiones que se pueden tomar usando este modelo, la decision que puede afectar mas a  un paciente, es que no se confirme el diagnostico en un caso donde deberia haberse confirmado, lo cual eventualmente puede resultar en la perdida de la vida de del paciente La *especificidad* del modelo es del 94.46%, así que podemos decir que el modelo tiene alrededor de un 5.54% de posibilidades de cometer este error, segun las metricas calculadas.

Asismismo, se puede clasificar un diganostico como confirmado en un caso donde no se puede confirmar el diagnostio,  lo cual podria resultar en un inconveniente para el paciente y para el centro medico, sin mencionar el gasto de fondos de los recursos de la CCSS para tratar una enfermedad inexistente. Desafortunadamente la *sensitividad*  no se pudo calcular, por lo cual no se conoce la probabilidad de cometer este error. Asimismo, la *precisión* es de un 0%, por lo que no se recomienda usar este modelo.

- **Para el modelo Arbol de decision**, se calcula una exactitud total del 94.46%, lo cual significa que el 94.46% de los valores fueron clasificados correctamente. Sin embargo, al igual que el modelo anterior, no se considera un modelo confiable debido al sesgo que presentan los datos.

Con respecto al tipo de decisiones que se pueden tomar usando este modelo, la decision que puede afectar mas a  un paciente, es que no se confirme el diagnostico en un caso donde deberia haberse confirmado, lo cual eventualmente puede resultar en la perdida de la vida de del paciente La *especificidad* del modelo es del 94.48%, así que podemos decir que el modelo tiene alrededor de un 5.52% de posibilidades de cometer este error, segun las metricas calculadas.

Asismismo, se puede clasificar un diganostico como confirmado en un caso donde no se puede confirmar el diagnostio,  lo cual podria resultar en un inconveniente para el paciente y para el centro medico, sin mencionar el gasto de fondos de los recursos de la CCSS para tratar una enfermedad inexistente. La *sensitividad*  del modelo es de un 44.44%, por lo  la probabilidad de cometer este error es de un 55.56%. Asimismo, la *precisión* es de un 0.3%, por lo que no se recomienda usar este modelo.

- Con base en los resultados obtenidos, se concluye que para todos los modelos el 94.46% de las observaciones clasificadas se hizo apropiadamente, lo cual significa una exactitud total del 94.46%.

- Asimismo, se concluye que el modelo con mayor precision es el arbol de decision, con un 0.3%, sin embargo como se observa claramente son modelos con una precision practicamente nula, por lo cual no se recomienda usarlos con este set de datos.

- Finalmente, basados en los resultados obtenidos se concluye que el modelo con mejores resultados es el arbol de decision.

## Calificación

* Desarrollo pregunta 25 %
    + Comprensión del problema
    + Desarrollo pregunta
* Analisis exloratorio 25 %
    + Comprensión de los datos
    + Decisión sobre metodos de analisis
* Modelado 25%
    + Preparación de los datos
    + Modelado
* Comparacion de modelos 25%
    + Evaluación de diferencias
    + Conclusiones
